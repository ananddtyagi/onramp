{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clone the repo\n",
    "# flatten the repo\n",
    "# go through each file and generate a summary for each file at the function level\n",
    "# store the summary, code\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mypy'...\n",
      "remote: Enumerating objects: 96389, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
      "remote: Total 96389 (delta 0), reused 0 (delta 0), pack-reused 96378\u001b[K\n",
      "Receiving objects: 100% (96389/96389), 64.87 MiB | 21.09 MiB/s, done.\n",
      "Resolving deltas: 100% (74700/74700), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/python/mypy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder\n",
    "folder_path = 'flat'\n",
    "\n",
    "# Get all the files in the folder and its subfolders\n",
    "files = glob.glob(os.path.join(folder_path, '**'), recursive=True)\n",
    "\n",
    "# Filter out directories and keep only files\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "\n",
    "# Move all the files to the flat folder\n",
    "flat_folder_path = 'flat'\n",
    "for file in files:\n",
    "    file_name = os.path.basename(file)\n",
    "    new_file_path = os.path.join(flat_folder_path, file_name)\n",
    "    os.rename(file, new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "flat_folder_path = 'flat'\n",
    "file_tuples = []\n",
    "\n",
    "for file_name in os.listdir(flat_folder_path):\n",
    "    file_path = os.path.join(flat_folder_path, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_text = file.read()\n",
    "            file_type = os.path.splitext(file_name)[1]\n",
    "            file_tuples.append((file_text, file_name, file_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\"python\",\n",
    "    chunk_size= 10000,\n",
    "    chunk_overlap= 1000,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text_list = [file_tuple[0] for file_tuple in file_tuples]\n",
    "file_info_list = [{'file_name': file_tuple[1], 'file_type': file_tuple[2]} for file_tuple in file_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonOutput = splitter.create_documents(file_text_list, file_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonOutput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from octoai.text_gen import ChatMessage, ChatCompletionResponseFormat\n",
    "from octoai.client import AsyncOctoAI, OctoAI\n",
    "from pydantic import BaseModel, Field\n",
    "import json \n",
    "import os\n",
    "\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    summaries: List[str]\n",
    "\n",
    "\n",
    "\n",
    "client = OctoAI(\n",
    "    api_key=os.getenv(\"OCTOAI_API_KEY\"),\n",
    ")        \n",
    "asnyc_client = AsyncOctoAI(\n",
    "    api_key=os.getenv(\"OCTOAI_API_KEY\"),\n",
    "    \n",
    ")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "\n",
    "sem = asyncio.Semaphore(4)\n",
    "\n",
    "async def generate_summaries(document):\n",
    "\tsem.acquire()\n",
    "\tresponse = await client.text_gen.create_chat_completion(\n",
    "\t\tmax_tokens=512,\n",
    "\t\tmessages=[\n",
    "\t\t\tChatMessage(\n",
    "\t\t\t\tcontent=\"You are an expert coder that creates summaries of code files.\",\n",
    "\t\t\t\trole=\"system\"\n",
    "\t\t\t),\n",
    "\t\t\tChatMessage(\n",
    "\t\t\t\tcontent=f\"Take this code file and create a list of summaries for each class and function in the block of code. Here is the code: {document}\",\n",
    "\t\t\t\trole=\"user\"\n",
    "\t\t\t)\n",
    "\t\t],\n",
    "\t\tmodel=\"mistral-7b-instruct-v0.3\",\n",
    "\t\tpresence_penalty=0,\n",
    "\t\ttemperature=0,\n",
    "\t\ttop_p=1,\n",
    "\t\tresponse_format=ChatCompletionResponseFormat(type='json_object', schema=Summary.model_json_schema()))\n",
    "\ttime.sleep(1)\n",
    "\tsem.release()\n",
    "\treturn json.loads(response.choices[0].message.content)['summaries']\n",
    "\n",
    "async def get_all_resps(pythonOutput):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        resps = await asyncio.gather(*[generate_summaries(doc) for doc in pythonOutput])\n",
    "        return resps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_resp(document):\n",
    "\tresponse = client.text_gen.create_chat_completion(\n",
    "\t\tmax_tokens=4000,\n",
    "\t\tmessages=[\n",
    "\t\t\tChatMessage(\n",
    "\t\t\t\tcontent=\"You are an expert coder that creates summaries of code files. Respond only following the provided json schema.\",\n",
    "\t\t\t\trole=\"system\"\n",
    "\t\t\t),\n",
    "\t\t\tChatMessage(\n",
    "\t\t\t\tcontent=f\"Take this code file and create a list of summaries for what this file does. Here is the code: {document.page_content}\",\n",
    "\t\t\t\trole=\"user\"\n",
    "\t\t\t)\n",
    "\t\t],\n",
    "\t\tmodel=\"meta-llama-3-8b-instruct\",\n",
    "\t\tpresence_penalty=0,\n",
    "\t\ttemperature=0,\n",
    "\t\ttop_p=1,\n",
    "\t\tresponse_format=ChatCompletionResponseFormat(type='json_object', schema=Summary.model_json_schema())\n",
    "  \t)\t\n",
    "\t# print(response.choices[0].message.content)\n",
    "\t# summaries = json.loads(response.choices[0].message.content)['summaries']\n",
    " \n",
    "\treturn response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandtyagi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name = 'hackathon'\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "all_summaries = []\n",
    "all_j_summ = []\n",
    "all_docs = []\n",
    "\n",
    "\n",
    "def add_docs_to_pinecone(docs, start):\n",
    "    all_summaries = [gen_resp(doc) for doc in docs]\n",
    "    all_j_summ = []\n",
    "    for i, sum in enumerate(all_summaries):\n",
    "        try:\n",
    "            all_j_summ.append(json.loads(sum)['summaries'])\n",
    "        except:\n",
    "            try:\n",
    "                all_j_summ.append(literal_eval('[' + sum.split('[')[1].split(']')[0] + ']'))\n",
    "            except:\n",
    "                all_j_summ.append([])\n",
    "                print(f'error for element {start + i}')\n",
    "    all_docs = []\n",
    "    for i, summ in enumerate(all_j_summ):\n",
    "        for s in summ:\n",
    "            doc = pythonOutput[i].copy()\n",
    "            doc.metadata['code'] = doc.page_content\n",
    "            doc.page_content = s\n",
    "            all_docs.append(doc)\n",
    "            \n",
    "    PineconeVectorStore.from_documents(\n",
    "        all_docs,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings,\n",
    "        namespace='codebase'\n",
    "    )\n",
    "    print(f\"Success upload: {start} to {start + len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success upload: 30 to 40\n",
      "error for element 46\n",
      "Success upload: 40 to 50\n",
      "error for element 52\n",
      "error for element 58\n",
      "Success upload: 50 to 60\n",
      "error for element 60\n",
      "error for element 61\n",
      "error for element 64\n",
      "error for element 65\n",
      "Success upload: 60 to 70\n",
      "error for element 72\n",
      "error for element 73\n",
      "error for element 74\n",
      "error for element 75\n",
      "error for element 79\n",
      "Success upload: 70 to 80\n",
      "Success upload: 80 to 90\n",
      "Success upload: 90 to 100\n",
      "Success upload: 100 to 110\n",
      "error for element 119\n",
      "Success upload: 110 to 120\n",
      "Success upload: 120 to 130\n",
      "Success upload: 130 to 140\n",
      "error for element 143\n",
      "error for element 144\n",
      "Success upload: 140 to 150\n",
      "Success upload: 150 to 160\n",
      "error for element 166\n",
      "error for element 167\n",
      "error for element 168\n",
      "Success upload: 160 to 170\n",
      "error for element 170\n",
      "error for element 173\n",
      "error for element 177\n",
      "error for element 178\n",
      "Success upload: 170 to 180\n",
      "error for element 180\n",
      "error for element 181\n",
      "error for element 182\n",
      "error for element 188\n",
      "Success upload: 180 to 190\n",
      "error for element 190\n",
      "error for element 194\n",
      "Success upload: 190 to 200\n",
      "Success upload: 200 to 210\n",
      "error for element 219\n",
      "Success upload: 210 to 220\n",
      "Success upload: 220 to 230\n",
      "Success upload: 230 to 240\n",
      "Success upload: 240 to 250\n",
      "error for element 250\n",
      "Success upload: 250 to 260\n",
      "Success upload: 260 to 270\n",
      "Success upload: 270 to 280\n",
      "Success upload: 280 to 290\n",
      "error for element 299\n",
      "Success upload: 290 to 300\n",
      "error for element 308\n",
      "Success upload: 300 to 310\n",
      "error for element 314\n",
      "Success upload: 310 to 320\n",
      "Success upload: 320 to 330\n",
      "Success upload: 330 to 340\n",
      "Success upload: 340 to 350\n",
      "error for element 357\n",
      "Success upload: 350 to 360\n",
      "Success upload: 360 to 370\n",
      "Success upload: 370 to 380\n",
      "Success upload: 380 to 390\n",
      "error for element 397\n",
      "Success upload: 390 to 400\n",
      "error for element 400\n",
      "Success upload: 400 to 410\n",
      "error for element 411\n",
      "Success upload: 410 to 420\n",
      "Success upload: 420 to 430\n",
      "error for element 433\n",
      "Success upload: 430 to 440\n",
      "Success upload: 440 to 450\n",
      "Success upload: 450 to 460\n",
      "Success upload: 460 to 470\n",
      "error for element 472\n",
      "Success upload: 470 to 480\n",
      "error for element 488\n",
      "error for element 489\n",
      "Success upload: 480 to 490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(pythonOutput):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43madd_docs_to_pinecone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpythonOutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     start \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "Cell \u001b[0;32mIn[98], line 23\u001b[0m, in \u001b[0;36madd_docs_to_pinecone\u001b[0;34m(docs, start)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_docs_to_pinecone\u001b[39m(docs, start):\n\u001b[0;32m---> 23\u001b[0m     all_summaries \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mgen_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m     all_j_summ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28msum\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_summaries):\n",
      "Cell \u001b[0;32mIn[98], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_docs_to_pinecone\u001b[39m(docs, start):\n\u001b[0;32m---> 23\u001b[0m     all_summaries \u001b[38;5;241m=\u001b[39m [\u001b[43mgen_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m     24\u001b[0m     all_j_summ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28msum\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_summaries):\n",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m, in \u001b[0;36mgen_resp\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_resp\u001b[39m(document):\n\u001b[0;32m----> 2\u001b[0m \tresponse \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert coder that creates summaries of code files. Respond only following the provided json schema.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTake this code file and create a list of summaries for what this file does. Here is the code: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama-3-8b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletionResponseFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \t\u001b[49m\u001b[43m)\u001b[49m\t\n\u001b[1;32m     20\u001b[0m \t\u001b[38;5;66;03m# print(response.choices[0].message.content)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \t\u001b[38;5;66;03m# summaries = json.loads(response.choices[0].message.content)['summaries']\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/octoai/text_gen/client.py:371\u001b[0m, in \u001b[0;36mTextGenClient.create_chat_completion\u001b[0;34m(self, messages, model, frequency_penalty, ignore_eos, logit_bias, loglikelihood, logprobs, max_tokens, n, peft, presence_penalty, repetition_penalty, response_format, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, request_options)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_chat_completion\u001b[39m(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     request_options: typing\u001b[38;5;241m.\u001b[39mOptional[RequestOptions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletionResponse:\n\u001b[1;32m    278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Create a Chat Completion.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     _response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_eos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_eos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloglikelihood\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloglikelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepetition_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43momit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOMIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/octoai/core/http_client.py:171\u001b[0m, in \u001b[0;36mHttpClient.request\u001b[0;34m(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\u001b[0m\n\u001b[1;32m    164\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_url(base_url)\n\u001b[1;32m    165\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m     request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_timeout\n\u001b[1;32m    169\u001b[0m )\n\u001b[0;32m--> 171\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_headers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                \u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_query_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                    \u001b[49m\u001b[43momit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_filter_request_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_filter_request_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_file_dict_to_httpx_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m max_retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_retry(response\u001b[38;5;241m=\u001b[39mresponse):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = 30\n",
    "batch_size = 10\n",
    "while start < len(pythonOutput):\n",
    "    add_docs_to_pinecone(pythonOutput[start:start + batch_size], start)\n",
    "    start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeded = embeddings.embed_documents(pythonOutput)\n",
    "\n",
    "vectorstore_from_docs = PineconeVectorStore.from_documents(\n",
    "        all_docs,\n",
    "        index_name=index_name,\n",
    "        embedding=embeddings,\n",
    "        namespace='codebase'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_j_summ = []\n",
    "for sum in all_summaries:\n",
    "    try:\n",
    "        all_j_summ.append(json.loads(sum)['summaries'])\n",
    "    except:\n",
    "        try:\n",
    "            all_j_summ.append(literal_eval('[' + sum.split('[')[1].split(']')[0] + ']'))\n",
    "        except:\n",
    "            all_j_summ.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandtyagi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    "pci = pc.Index('hackathon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pythonOutput[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_j_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for i, summ in enumerate(all_j_summ):\n",
    "    \n",
    "    for s in summ:\n",
    "        doc = pythonOutput[i].copy()\n",
    "        doc.metadata['code'] = doc.page_content\n",
    "        doc.page_content = s\n",
    "        all_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Defines a ContextVar class for managing context variables in a program.', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi', 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', 'code': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}),\n",
       " Document(page_content='Provides a Token class for managing tokens in a program.', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi', 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', 'code': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}),\n",
       " Document(page_content='Creates a Context class for managing context variables and tokens in a program.', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi', 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', 'code': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}),\n",
       " Document(page_content='Provides a copy_context function for copying a context.', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi', 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', 'code': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}),\n",
       " Document(page_content='Provides a way to get and set context variables and tokens in a program.', metadata={'file_name': 'contextvars.pyi', 'file_type': '.pyi', 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...', 'code': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}),\n",
       " Document(page_content='Defines a WSGI application validator', metadata={'file_name': 'validate.pyi', 'file_type': '.pyi', 'text': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...', 'code': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...'}),\n",
       " Document(page_content='Provides input and error wrappers for WSGI applications', metadata={'file_name': 'validate.pyi', 'file_type': '.pyi', 'text': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...', 'code': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...'}),\n",
       " Document(page_content='Creates input, error, and write wrappers for WSGI applications', metadata={'file_name': 'validate.pyi', 'file_type': '.pyi', 'text': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...', 'code': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...'}),\n",
       " Document(page_content='Creates partial and full iterators for WSGI applications', metadata={'file_name': 'validate.pyi', 'file_type': '.pyi', 'text': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...', 'code': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...'}),\n",
       " Document(page_content='Provides a way to check if an iterator has been closed', metadata={'file_name': 'validate.pyi', 'file_type': '.pyi', 'text': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...', 'code': 'from _typeshed.wsgi import ErrorStream, InputStream, WSGIApplication\\nfrom collections.abc import Callable, Iterable, Iterator\\nfrom typing import Any, NoReturn\\nfrom typing_extensions import TypeAlias\\n\\n__all__ = [\"validator\"]\\n\\nclass WSGIWarning(Warning): ...\\n\\ndef validator(application: WSGIApplication) -> WSGIApplication: ...\\n\\nclass InputWrapper:\\n    input: InputStream\\n    def __init__(self, wsgi_input: InputStream) -> None: ...\\n    def read(self, size: int) -> bytes: ...\\n    def readline(self, size: int = ...) -> bytes: ...\\n    def readlines(self, hint: int = ...) -> bytes: ...\\n    def __iter__(self) -> Iterator[bytes]: ...\\n    def close(self) -> NoReturn: ...\\n\\nclass ErrorWrapper:\\n    errors: ErrorStream\\n    def __init__(self, wsgi_errors: ErrorStream) -> None: ...\\n    def write(self, s: str) -> None: ...\\n    def flush(self) -> None: ...\\n    def writelines(self, seq: Iterable[str]) -> None: ...\\n    def close(self) -> NoReturn: ...\\n\\n_WriterCallback: TypeAlias = Callable[[bytes], Any]\\n\\nclass WriteWrapper:\\n    writer: _WriterCallback\\n    def __init__(self, wsgi_writer: _WriterCallback) -> None: ...\\n    def __call__(self, s: bytes) -> None: ...\\n\\nclass PartialIteratorWrapper:\\n    iterator: Iterator[bytes]\\n    def __init__(self, wsgi_iterator: Iterator[bytes]) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n\\nclass IteratorWrapper:\\n    original_iterator: Iterator[bytes]\\n    iterator: Iterator[bytes]\\n    closed: bool\\n    check_start_response: bool | None\\n    def __init__(self, wsgi_iterator: Iterator[bytes], check_start_response: bool | None) -> None: ...\\n    def __iter__(self) -> IteratorWrapper: ...\\n    def __next__(self) -> bytes: ...\\n    def close(self) -> None: ...\\n    def __del__(self) -> None: ...'}),\n",
       " Document(page_content='Converts a URL to a pathname', metadata={'file_name': 'nturl2path.pyi', 'file_type': '.pyi', 'text': 'def url2pathname(url: str) -> str: ...\\ndef pathname2url(p: str) -> str: ...', 'code': 'def url2pathname(url: str) -> str: ...\\ndef pathname2url(p: str) -> str: ...'}),\n",
       " Document(page_content='Converts a pathname to a URL', metadata={'file_name': 'nturl2path.pyi', 'file_type': '.pyi', 'text': 'def url2pathname(url: str) -> str: ...\\ndef pathname2url(p: str) -> str: ...', 'code': 'def url2pathname(url: str) -> str: ...\\ndef pathname2url(p: str) -> str: ...'}),\n",
       " Document(page_content='Enforcement of upper bounds for generic functions and classes', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Using information from upper bounds for generic functions and classes', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for upper bounds on generic functions and classes', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for generic functions and classes with void return types', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for generic functions and classes with any return types', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for generic functions and classes with higher-order functions', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for inheritance and upper bounds', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds in method calls', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds in class method calls', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds in static method calls', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds in decorator', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='Test cases for using information from upper bounds in iterable unpacking', metadata={'file_name': 'check-bound.test', 'file_type': '.test', 'text': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]', 'code': '-- Enforcement of upper bounds\\n-- ---------------------------\\n\\n\\n[case testBoundOnGenericFunction]\\nfrom typing import TypeVar\\n\\nclass A: pass\\nclass B(A): pass\\nclass C(A): pass\\nclass D: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\nU = TypeVar(\\'U\\')\\ndef f(x: T) -> T: pass\\ndef g(x: U) -> U:\\n    return f(x) # E: Value of type variable \"T\" of \"f\" cannot be \"U\"\\n\\nf(A())\\nf(B())\\nf(D()) # E: Value of type variable \"T\" of \"f\" cannot be \"D\"\\n\\nb = B()\\nif int():\\n    b = f(b)\\nif int():\\n    b = f(C()) # E: Incompatible types in assignment (expression has type \"C\", variable has type \"B\")\\n\\n\\n[case testBoundOnGenericClass]\\nfrom typing import TypeVar, Generic\\n\\nclass A: pass\\nclass B(A): pass\\nT = TypeVar(\\'T\\', bound=A)\\n\\nclass G(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\n\\nv: G[A]\\nw: G[B]\\nx: G[str]  # E: Type argument \"str\" of \"G\" must be a subtype of \"A\"\\ny = G(\\'a\\') # E: Value of type variable \"T\" of \"G\" cannot be \"str\"\\nz = G(A())\\nz = G(B())\\n\\n\\n[case testBoundVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    t = None # type: T\\n    def get(self) -> T:\\n        return self.t\\nc1 = None # type: C[None]\\nc1.get()\\nd = c1.get()\\nreveal_type(d)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundAny]\\nfrom typing import TypeVar, Generic\\nT = TypeVar(\\'T\\', bound=int)\\nclass C(Generic[T]):\\n    def __init__(self, x: T) -> None: pass\\ndef f(x: T) -> T:\\n    return x\\n\\ndef g(): pass\\n\\nf(g())\\nC(g())\\nz: C\\n\\n\\n[case testBoundHigherOrderWithVoid]\\n# flags: --no-strict-optional\\nfrom typing import TypeVar, Callable\\nclass A: pass\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(g: Callable[[], T]) -> T:\\n    return g()\\ndef h() -> None: pass\\nf(h)\\na = f(h)\\nreveal_type(a)  # N: Revealed type is \"None\"\\n\\n\\n[case testBoundInheritance]\\nfrom typing import TypeVar, Generic\\nclass A: pass\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\n\\nclass C(Generic[TA]): pass\\nclass D0(C[TA], Generic[TA]): pass\\nclass D1(C[T], Generic[T]): pass # E: Type argument \"T\" of \"C\" must be a subtype of \"A\"\\nclass D2(C[A]): pass\\nclass D3(C[str]): pass # E: Type argument \"str\" of \"C\" must be a subtype of \"A\"\\n\\n\\n-- Using information from upper bounds\\n-- -----------------------------------\\n\\n\\n[case testBoundGenericFunctions]\\nfrom typing import TypeVar\\nclass A: pass\\nclass B(A): pass\\n\\nT = TypeVar(\\'T\\')\\nTA = TypeVar(\\'TA\\', bound=A)\\nTB = TypeVar(\\'TB\\', bound=B)\\n\\ndef f(x: T) -> T:\\n    return x\\ndef g(x: TA) -> TA:\\n    return f(x)\\ndef h(x: TB) -> TB:\\n    return g(x)\\ndef g2(x: TA) -> TA:\\n    return h(x) # Fail\\n\\ndef j(x: TA) -> A:\\n    return x\\ndef k(x: TA) -> B:\\n    return x # Fail\\n[out]\\nmain:16: error: Value of type variable \"TB\" of \"h\" cannot be \"TA\"\\nmain:21: error: Incompatible return value type (got \"TA\", expected \"B\")\\n\\n\\n[case testBoundMethodUsage]\\nfrom typing import TypeVar\\nclass A0:\\n    def foo(self) -> None: pass\\nclass A(A0):\\n    def bar(self) -> None: pass\\n    a = 1\\n    @property\\n    def b(self) -> int:\\n        return self.a\\nclass B(A):\\n    def baz(self) -> None: pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\n\\ndef f(x: T) -> T:\\n    x.foo()\\n    x.bar()\\n    x.baz()  # E: \"T\" has no attribute \"baz\"\\n    x.a\\n    x.b\\n    return x\\n\\nb = f(B())\\n[builtins fixtures/property.pyi]\\n[out]\\n\\n[case testBoundClassMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @classmethod\\n    def foo(cls, x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundClassMethodWithNamedTupleBase]\\nfrom typing import NamedTuple, Type, TypeVar\\nclass A(NamedTuple):\\n    @classmethod\\n    def foo(cls) -> None: ...\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: Type[T]) -> None:\\n    reveal_type(x.foo)  # N: Revealed type is \"def ()\"\\n    x.foo()\\n[builtins fixtures/classmethod.pyi]\\n\\n\\n[case testBoundStaticMethod]\\nfrom typing import TypeVar\\nclass A0:\\n    @staticmethod\\n    def foo(x: int) -> int: pass\\nclass A(A0): pass\\n\\nT = TypeVar(\\'T\\', bound=A)\\ndef f(x: T) -> int:\\n    return x.foo(22)\\n[builtins fixtures/staticmethod.pyi]\\n\\n\\n[case testBoundOnDecorator]\\nfrom typing import TypeVar, Callable, Any, cast\\nT = TypeVar(\\'T\\', bound=Callable[..., Any])\\n\\ndef twice(f: T) -> T:\\n    def result(*args, **kwargs) -> Any:\\n        f(*args, **kwargs)\\n        return f(*args, **kwargs)\\n    return cast(T, result)\\n\\n@twice\\ndef foo(x: int) -> int:\\n    return x\\n\\na = 1\\nb = foo(a)\\nif int():\\n    b = \\'a\\' # E: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")\\ntwice(a) # E: Value of type variable \"T\" of \"twice\" cannot be \"int\"\\n[builtins fixtures/args.pyi]\\n\\n\\n[case testIterableBoundUnpacking]\\nfrom typing import Tuple, TypeVar\\nTupleT = TypeVar(\"TupleT\", bound=Tuple[int, ...])\\ndef f(t: TupleT) -> None:\\n    a, *b = t\\n    reveal_type(a)  # N: Revealed type is \"builtins.int\"\\n    reveal_type(b)  # N: Revealed type is \"builtins.list[builtins.int]\"\\n[builtins fixtures/tuple.pyi]'}),\n",
       " Document(page_content='This code file describes the fine-grained incremental daemon mode of the mypy build system, which updates targets in other modules that may be affected by externally-visible changes in the changed modules.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")', 'code': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")'}),\n",
       " Document(page_content='The fine-grained incremental daemon mode keeps program state in memory between incremental runs, processing only changed modules and their dependencies, and allowing for independent processing of individual modules within an import cycle.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")', 'code': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")'}),\n",
       " Document(page_content='The code file explains the concepts of targets, triggers, and fine-grained dependencies, and provides a detailed overview of the fine-grained incremental program update process, including determining changed modules, processing changed modules, and firing triggers.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")', 'code': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")'}),\n",
       " Document(page_content='The code file also includes functions and classes for processing fine-grained dependencies, merging ASTs, and stripping stale AST nodes, and provides an overview of the testing process using end-to-end fine-grained incremental mode test cases.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")', 'code': '\"\"\"Update build by processing changes using fine-grained dependencies.\\n\\nUse fine-grained dependencies to update targets in other modules that\\nmay be affected by externally-visible changes in the changed modules.\\n\\nThis forms the core of the fine-grained incremental daemon mode. This\\nmodule is not used at all by the \\'classic\\' (non-daemon) incremental\\nmode.\\n\\nHere is some motivation for this mode:\\n\\n* By keeping program state in memory between incremental runs, we\\n  only have to process changed modules, not their dependencies. The\\n  classic incremental mode has to deserialize the symbol tables of\\n  all dependencies of changed modules, which can be slow for large\\n  programs.\\n\\n* Fine-grained dependencies allow processing only the relevant parts\\n  of modules indirectly affected by a change. Say, if only one function\\n  in a large module is affected by a change in another module, only this\\n  function is processed. The classic incremental mode always processes\\n  an entire file as a unit, which is typically much slower.\\n\\n* It\\'s possible to independently process individual modules within an\\n  import cycle (SCC). Small incremental changes can be fast independent\\n  of the size of the related SCC. In classic incremental mode, any change\\n  within a SCC requires the entire SCC to be processed, which can slow\\n  things down considerably.\\n\\nSome terms:\\n\\n* A *target* is a function/method definition or the top level of a module.\\n  We refer to targets using their fully qualified name (e.g.\\n  \\'mod.Cls.method\\'). Targets are the smallest units of processing during\\n  fine-grained incremental checking.\\n\\n* A *trigger* represents the properties of a part of a program, and it\\n  gets triggered/fired when these properties change. For example,\\n  \\'<mod.func>\\' refers to a module-level function. It gets triggered if\\n  the signature of the function changes, or if the function is removed,\\n  for example.\\n\\nSome program state is maintained across multiple build increments in\\nmemory:\\n\\n* The full ASTs of all modules are stored in memory all the time (this\\n  includes the type map).\\n\\n* A fine-grained dependency map is maintained, which maps triggers to\\n  affected program locations (these can be targets, triggers, or\\n  classes). The latter determine what other parts of a program need to\\n  be processed again due to a fired trigger.\\n\\nHere\\'s a summary of how a fine-grained incremental program update happens:\\n\\n* Determine which modules have changes in their source code since the\\n  previous update.\\n\\n* Process changed modules one at a time. Perform a separate full update\\n  for each changed module, but only report the errors after all modules\\n  have been processed, since the intermediate states can generate bogus\\n  errors due to only seeing a partial set of changes.\\n\\n* Each changed module is processed in full. We parse the module, and\\n  run semantic analysis to create a new AST and symbol table for the\\n  module. Reuse the existing ASTs and symbol tables of modules that\\n  have no changes in their source code. At the end of this stage, we have\\n  two ASTs and symbol tables for the changed module (the old and the new\\n  versions). The latter AST has not yet been type checked.\\n\\n* Take a snapshot of the old symbol table. This is used later to determine\\n  which properties of the module have changed and which triggers to fire.\\n\\n* Merge the old AST with the new AST, preserving the identities of\\n  externally visible AST nodes for which we can find a corresponding node\\n  in the new AST. (Look at mypy.server.astmerge for the details.) This\\n  way all external references to AST nodes in the changed module will\\n  continue to point to the right nodes (assuming they still have a valid\\n  target).\\n\\n* Type check the new module.\\n\\n* Take another snapshot of the symbol table of the changed module.\\n  Look at the differences between the old and new snapshots to determine\\n  which parts of the changed modules have changed. The result is a set of\\n  fired triggers.\\n\\n* Using the dependency map and the fired triggers, decide which other\\n  targets have become stale and need to be reprocessed.\\n\\n* Create new fine-grained dependencies for the changed module. We don\\'t\\n  garbage collect old dependencies, since extra dependencies are relatively\\n  harmless (they take some memory and can theoretically slow things down\\n  a bit by causing redundant work). This is implemented in\\n  mypy.server.deps.\\n\\n* Strip the stale AST nodes that we found above. This returns them to a\\n  state resembling the end of semantic analysis pass 1. We\\'ll run semantic\\n  analysis again on the existing AST nodes, and since semantic analysis\\n  is not idempotent, we need to revert some changes made during semantic\\n  analysis. This is implemented in mypy.server.aststrip.\\n\\n* Run semantic analyzer passes 2 and 3 on the stale AST nodes, and type\\n  check them. We also need to do the symbol table snapshot comparison\\n  dance to find any changes, and we need to merge ASTs to preserve AST node\\n  identities.\\n\\n* If some triggers haven been fired, continue processing and repeat the\\n  previous steps until no triggers are fired.\\n\\nThis is module is tested using end-to-end fine-grained incremental mode\\ntest cases (test-data/unit/fine-grained*.test).\\n\"\"\"\\n\\nfrom __future__ import annotations\\n\\nimport os\\nimport re\\nimport sys\\nimport time\\nfrom typing import Callable, Final, NamedTuple, Sequence, Union\\nfrom typing_extensions import TypeAlias as _TypeAlias\\n\\nfrom mypy.build import (\\n    DEBUG_FINE_GRAINED,\\n    FAKE_ROOT_MODULE,\\n    BuildManager,\\n    BuildResult,\\n    Graph,\\n    State,\\n    load_graph,\\n    process_fresh_modules,\\n)\\nfrom mypy.checker import FineGrainedDeferredNode\\nfrom mypy.errors import CompileError\\nfrom mypy.fscache import FileSystemCache\\nfrom mypy.modulefinder import BuildSource\\nfrom mypy.nodes import (\\n    Decorator,\\n    FuncDef,\\n    ImportFrom,\\n    MypyFile,\\n    OverloadedFuncDef,\\n    SymbolNode,\\n    SymbolTable,\\n    TypeInfo,\\n)\\nfrom mypy.options import Options\\nfrom mypy.semanal_main import (\\n    core_modules,\\n    semantic_analysis_for_scc,\\n    semantic_analysis_for_targets,\\n)\\nfrom mypy.server.astdiff import (\\n    SymbolSnapshot,\\n    compare_symbol_table_snapshots,\\n    snapshot_symbol_table,\\n)\\nfrom mypy.server.astmerge import merge_asts\\nfrom mypy.server.aststrip import SavedAttributes, strip_target\\nfrom mypy.server.deps import get_dependencies_of_target, merge_dependencies\\nfrom mypy.server.target import trigger_to_target\\nfrom mypy.server.trigger import WILDCARD_TAG, make_trigger\\nfrom mypy.typestate import type_state\\nfrom mypy.util import module_prefix, split_target\\n\\nMAX_ITER: Final = 1000\\n\\nSENSITIVE_INTERNAL_MODULES = tuple(core_modules) + (\"mypy_extensions\", \"typing_extensions\")'}),\n",
       " Document(page_content='The FineGrainedBuildManager class is used to manage fine-grained builds based on a batch build.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The class initializes with a result from the initialized build, including the manager and graph.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The update method processes changed modules, propagates changes to other modules, and reuses the original BuildManager and Graph.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The trigger method triggers a specific target explicitly, intended for use by the suggestions engine.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The flush_cache method flushes the AST cache, which needs to be called after each increment to detect file changes reliably.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The update_one method processes a module from the list of changed modules, returning the updated list of pending changed modules, the module that was processed, and if there was a blocking error, the error messages from it.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='The update_module method updates a single modified module, processing one of the new modules if it contains imports of previously unseen modules and returning the remaining work to be done.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:', 'code': 'class FineGrainedBuildManager:\\n    def __init__(self, result: BuildResult) -> None:\\n        \"\"\"Initialize fine-grained build based on a batch build.\\n\\n        Args:\\n            result: Result from the initialized build.\\n                    The manager and graph will be taken over by this class.\\n            manager: State of the build (mutated by this class)\\n            graph: Additional state of the build (mutated by this class)\\n        \"\"\"\\n        manager = result.manager\\n        self.manager = manager\\n        self.graph = result.graph\\n        self.previous_modules = get_module_to_path_map(self.graph)\\n        self.deps = manager.fg_deps\\n        # Merge in any root dependencies that may not have been loaded\\n        merge_dependencies(manager.load_fine_grained_deps(FAKE_ROOT_MODULE), self.deps)\\n        self.previous_targets_with_errors = manager.errors.targets()\\n        self.previous_messages: list[str] = result.errors.copy()\\n        # Module, if any, that had blocking errors in the last run as (id, path) tuple.\\n        self.blocking_error: tuple[str, str] | None = None\\n        # Module that we haven\\'t processed yet but that are known to be stale.\\n        self.stale: list[tuple[str, str]] = []\\n        # Disable the cache so that load_graph doesn\\'t try going back to disk\\n        # for the cache.\\n        self.manager.cache_enabled = False\\n\\n        # Some hints to the test suite about what is going on:\\n        # Active triggers during the last update\\n        self.triggered: list[str] = []\\n        # Modules passed to update during the last update\\n        self.changed_modules: list[tuple[str, str]] = []\\n        # Modules processed during the last update\\n        self.updated_modules: list[str] = []\\n        # Targets processed during last update (for testing only).\\n        self.processed_targets: list[str] = []\\n\\n    def update(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        removed_modules: list[tuple[str, str]],\\n        followed: bool = False,\\n    ) -> list[str]:\\n        \"\"\"Update previous build result by processing changed modules.\\n\\n        Also propagate changes to other modules as needed, but only process\\n        those parts of other modules that are affected by the changes. Retain\\n        the existing ASTs and symbol tables of unaffected modules.\\n\\n        Reuses original BuildManager and Graph.\\n\\n        Args:\\n            changed_modules: Modules changed since the previous update/build; each is\\n                a (module id, path) tuple. Includes modified and added modules.\\n                Assume this is correct; it\\'s not validated here.\\n            removed_modules: Modules that have been deleted since the previous update\\n                or removed from the build.\\n            followed: If True, the modules were found through following imports\\n\\n        Returns:\\n            A list of errors.\\n        \"\"\"\\n        self.processed_targets.clear()\\n        changed_modules = changed_modules + removed_modules\\n        removed_set = {module for module, _ in removed_modules}\\n        self.changed_modules = changed_modules\\n\\n        if not changed_modules:\\n            return self.previous_messages\\n\\n        # Reset find_module\\'s caches for the new build.\\n        self.manager.find_module_cache.clear()\\n\\n        self.triggered = []\\n        self.updated_modules = []\\n        changed_modules = dedupe_modules(changed_modules + self.stale)\\n        initial_set = {id for id, _ in changed_modules}\\n        self.manager.log_fine_grained(\\n            \"==== update %s ====\" % \", \".join(repr(id) for id, _ in changed_modules)\\n        )\\n        if self.previous_targets_with_errors and is_verbose(self.manager):\\n            self.manager.log_fine_grained(\\n                \"previous targets with errors: %s\" % sorted(self.previous_targets_with_errors)\\n            )\\n\\n        blocking_error = None\\n        if self.blocking_error:\\n            # Handle blocking errors first. We\\'ll exit as soon as we find a\\n            # module that still has blocking errors.\\n            self.manager.log_fine_grained(f\"existing blocker: {self.blocking_error[0]}\")\\n            changed_modules = dedupe_modules([self.blocking_error] + changed_modules)\\n            blocking_error = self.blocking_error[0]\\n            self.blocking_error = None\\n\\n        while True:\\n            result = self.update_one(\\n                changed_modules, initial_set, removed_set, blocking_error, followed\\n            )\\n            changed_modules, (next_id, next_path), blocker_messages = result\\n\\n            if blocker_messages is not None:\\n                self.blocking_error = (next_id, next_path)\\n                self.stale = changed_modules\\n                messages = blocker_messages\\n                break\\n\\n            # It looks like we are done processing everything, so now\\n            # reprocess all targets with errors. We are careful to\\n            # support the possibility that reprocessing an errored module\\n            # might trigger loading of a module, but I am not sure\\n            # if this can really happen.\\n            if not changed_modules:\\n                # N.B: We just checked next_id, so manager.errors contains\\n                # the errors from it. Thus we consider next_id up to date\\n                # when propagating changes from the errored targets,\\n                # which prevents us from reprocessing errors in it.\\n                changed_modules = propagate_changes_using_dependencies(\\n                    self.manager,\\n                    self.graph,\\n                    self.deps,\\n                    set(),\\n                    {next_id},\\n                    self.previous_targets_with_errors,\\n                    self.processed_targets,\\n                )\\n                changed_modules = dedupe_modules(changed_modules)\\n                if not changed_modules:\\n                    # Preserve state needed for the next update.\\n                    self.previous_targets_with_errors = self.manager.errors.targets()\\n                    messages = self.manager.errors.new_messages()\\n                    break\\n\\n        messages = sort_messages_preserving_file_order(messages, self.previous_messages)\\n        self.previous_messages = messages.copy()\\n        return messages\\n\\n    def trigger(self, target: str) -> list[str]:\\n        \"\"\"Trigger a specific target explicitly.\\n\\n        This is intended for use by the suggestions engine.\\n        \"\"\"\\n        self.manager.errors.reset()\\n        changed_modules = propagate_changes_using_dependencies(\\n            self.manager,\\n            self.graph,\\n            self.deps,\\n            set(),\\n            set(),\\n            self.previous_targets_with_errors | {target},\\n            [],\\n        )\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors = self.manager.errors.targets()\\n        self.previous_messages = self.manager.errors.new_messages().copy()\\n        return self.update(changed_modules, [])\\n\\n    def flush_cache(self) -> None:\\n        \"\"\"Flush AST cache.\\n\\n        This needs to be called after each increment, or file changes won\\'t\\n        be detected reliably.\\n        \"\"\"\\n        self.manager.ast_cache.clear()\\n\\n    def update_one(\\n        self,\\n        changed_modules: list[tuple[str, str]],\\n        initial_set: set[str],\\n        removed_set: set[str],\\n        blocking_error: str | None,\\n        followed: bool,\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Process a module from the list of changed modules.\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Updated list of pending changed modules as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        t0 = time.time()\\n        next_id, next_path = changed_modules.pop(0)\\n\\n        # If we have a module with a blocking error that is no longer\\n        # in the import graph, we must skip it as otherwise we\\'ll be\\n        # stuck with the blocking error.\\n        if (\\n            next_id == blocking_error\\n            and next_id not in self.previous_modules\\n            and next_id not in initial_set\\n        ):\\n            self.manager.log_fine_grained(\\n                f\"skip {next_id!r} (module with blocking error not in import graph)\"\\n            )\\n            return changed_modules, (next_id, next_path), None\\n\\n        result = self.update_module(next_id, next_path, next_id in removed_set, followed)\\n        remaining, (next_id, next_path), blocker_messages = result\\n        changed_modules = [(id, path) for id, path in changed_modules if id != next_id]\\n        changed_modules = dedupe_modules(remaining + changed_modules)\\n        t1 = time.time()\\n\\n        self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:'}),\n",
       " Document(page_content='Updates a single modified module, processing one new module and returning remaining work to be done.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content='Logs fine-grained messages for module updates, including time taken and number of changed modules.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content='Handles sensitive internal modules by skipping updates and returning no remaining work.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content='Updates module symbol table snapshots and triggers active triggers after processing a module.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content='Calculates active triggers and logs triggered modules, and updates triggered and processed targets.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content='Adds stats for update and propagate times, and preserves state for the next update.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None', 'code': 'self.manager.log_fine_grained(\\n            f\"update once: {next_id} in {t1 - t0:.3f}s - {len(changed_modules)} left\"\\n        )\\n\\n        return changed_modules, (next_id, next_path), blocker_messages\\n\\n    def update_module(\\n        self, module: str, path: str, force_removed: bool, followed: bool\\n    ) -> tuple[list[tuple[str, str]], tuple[str, str], list[str] | None]:\\n        \"\"\"Update a single modified module.\\n\\n        If the module contains imports of previously unseen modules, only process one of\\n        the new modules and return the remaining work to be done.\\n\\n        Args:\\n            module: Id of the module\\n            path: File system path of the module\\n            force_removed: If True, consider module removed from the build even if path\\n                exists (used for removing an existing file from the build)\\n            followed: Was this found via import following?\\n\\n        Returns:\\n            Tuple with these items:\\n\\n            - Remaining modules to process as (module id, path) tuples\\n            - Module which was actually processed as (id, path) tuple\\n            - If there was a blocking error, the error messages from it\\n        \"\"\"\\n        self.manager.log_fine_grained(f\"--- update single {module!r} ---\")\\n        self.updated_modules.append(module)\\n\\n        # builtins and friends could potentially get triggered because\\n        # of protocol stuff, but nothing good could possibly come from\\n        # actually updating them.\\n        if module in SENSITIVE_INTERNAL_MODULES:\\n            return [], (module, path), None\\n\\n        manager = self.manager\\n        previous_modules = self.previous_modules\\n        graph = self.graph\\n\\n        ensure_deps_loaded(module, self.deps, graph)\\n\\n        # If this is an already existing module, make sure that we have\\n        # its tree loaded so that we can snapshot it for comparison.\\n        ensure_trees_loaded(manager, graph, [module])\\n\\n        t0 = time.time()\\n        # Record symbol table snapshot of old version the changed module.\\n        old_snapshots: dict[str, dict[str, SymbolSnapshot]] = {}\\n        if module in manager.modules:\\n            snapshot = snapshot_symbol_table(module, manager.modules[module].names)\\n            old_snapshots[module] = snapshot\\n\\n        manager.errors.reset()\\n        self.processed_targets.append(module)\\n        result = update_module_isolated(\\n            module, path, manager, previous_modules, graph, force_removed, followed\\n        )\\n        if isinstance(result, BlockedUpdate):\\n            # Blocking error -- just give up\\n            module, path, remaining, errors = result\\n            self.previous_modules = get_module_to_path_map(graph)\\n            return remaining, (module, path), errors\\n        assert isinstance(result, NormalUpdate)  # Work around #4124\\n        module, path, remaining, tree = result\\n\\n        # TODO: What to do with stale dependencies?\\n        t1 = time.time()\\n        triggered = calculate_active_triggers(manager, old_snapshots, {module: tree})\\n        if is_verbose(self.manager):\\n            filtered = [trigger for trigger in triggered if not trigger.endswith(\"__>\")]\\n            self.manager.log_fine_grained(f\"triggered: {sorted(filtered)!r}\")\\n        self.triggered.extend(triggered | self.previous_targets_with_errors)\\n        if module in graph:\\n            graph[module].update_fine_grained_deps(self.deps)\\n            graph[module].free_state()\\n        remaining += propagate_changes_using_dependencies(\\n            manager,\\n            graph,\\n            self.deps,\\n            triggered,\\n            {module},\\n            targets_with_errors=set(),\\n            processed_targets=self.processed_targets,\\n        )\\n        t2 = time.time()\\n        manager.add_stats(update_isolated_time=t1 - t0, propagate_time=t2 - t1)\\n\\n        # Preserve state needed for the next update.\\n        self.previous_targets_with_errors.update(manager.errors.targets())\\n        self.previous_modules = get_module_to_path_map(graph)\\n\\n        return remaining, (module, path), None'}),\n",
       " Document(page_content=\"Finds all dependencies of nodes in the initial set that haven't had their tree loaded.\", metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)', 'code': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)'}),\n",
       " Document(page_content='Ensures that dependencies on a module are loaded, including dependencies from parent modules.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)', 'code': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)'}),\n",
       " Document(page_content='Ensures that modules in the initial set and their dependencies have loaded trees.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)', 'code': 'def find_unloaded_deps(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> list[str]:\\n    \"\"\"Find all the deps of the nodes in initial that haven\\'t had their tree loaded.\\n\\n    The key invariant here is that if a module is loaded, so are all\\n    of their dependencies. This means that when we encounter a loaded\\n    module, we don\\'t need to explore its dependencies.  (This\\n    invariant is slightly violated when dependencies are added, which\\n    can be handled by calling find_unloaded_deps directly on the new\\n    dependencies.)\\n    \"\"\"\\n    worklist = list(initial)\\n    seen: set[str] = set()\\n    unloaded = []\\n    while worklist:\\n        node = worklist.pop()\\n        if node in seen or node not in graph:\\n            continue\\n        seen.add(node)\\n        if node not in manager.modules:\\n            ancestors = graph[node].ancestors or []\\n            worklist.extend(graph[node].dependencies + ancestors)\\n            unloaded.append(node)\\n\\n    return unloaded\\n\\n\\ndef ensure_deps_loaded(module: str, deps: dict[str, set[str]], graph: dict[str, State]) -> None:\\n    \"\"\"Ensure that the dependencies on a module are loaded.\\n\\n    Dependencies are loaded into the \\'deps\\' dictionary.\\n\\n    This also requires loading dependencies from any parent modules,\\n    since dependencies will get stored with parent modules when a module\\n    doesn\\'t exist.\\n    \"\"\"\\n    if module in graph and graph[module].fine_grained_deps_loaded:\\n        return\\n    parts = module.split(\".\")\\n    for i in range(len(parts)):\\n        base = \".\".join(parts[: i + 1])\\n        if base in graph and not graph[base].fine_grained_deps_loaded:\\n            merge_dependencies(graph[base].load_fine_grained_deps(), deps)\\n            graph[base].fine_grained_deps_loaded = True\\n\\n\\ndef ensure_trees_loaded(\\n    manager: BuildManager, graph: dict[str, State], initial: Sequence[str]\\n) -> None:\\n    \"\"\"Ensure that the modules in initial and their deps have loaded trees.\"\"\"\\n    to_process = find_unloaded_deps(manager, graph, initial)\\n    if to_process:\\n        if is_verbose(manager):\\n            manager.log_fine_grained(\\n                \"Calling process_fresh_modules on set of size {} ({})\".format(\\n                    len(to_process), sorted(to_process)\\n                )\\n            )\\n        process_fresh_modules(graph, to_process, manager)\\n\\n\\n# The result of update_module_isolated when no blockers, with these items:\\n#\\n# - Id of the changed module (can be different from the module argument)\\n# - Path of the changed module\\n# - New AST for the changed module (None if module was deleted)\\n# - Remaining changed modules that are not processed yet as (module id, path)\\n#   tuples (non-empty if the original changed module imported other new\\n#   modules)'}),\n",
       " Document(page_content='Defines a class named NormalUpdate, a subclass of NamedTuple, with attributes module, path, remaining, and tree.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class NormalUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    tree: MypyFile | None\\n\\n\\n# The result of update_module_isolated when there is a blocking error. Items\\n# are similar to NormalUpdate (but there are fewer).', 'code': 'class NormalUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    tree: MypyFile | None\\n\\n\\n# The result of update_module_isolated when there is a blocking error. Items\\n# are similar to NormalUpdate (but there are fewer).'}),\n",
       " Document(page_content='Describes the result of update_module_isolated when there is a blocking error, with items similar to NormalUpdate but with fewer attributes.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class NormalUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    tree: MypyFile | None\\n\\n\\n# The result of update_module_isolated when there is a blocking error. Items\\n# are similar to NormalUpdate (but there are fewer).', 'code': 'class NormalUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    tree: MypyFile | None\\n\\n\\n# The result of update_module_isolated when there is a blocking error. Items\\n# are similar to NormalUpdate (but there are fewer).'}),\n",
       " Document(page_content='The code defines a class `BlockedUpdate` and a function `update_module_isolated` that updates a single module in a build process.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The `update_module_isolated` function takes several parameters, including the module to update, its path, a build manager, a graph of modules, and a flag to force removal of the module.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='It returns a named tuple describing the result of the update, which can be either a normal update or a blocked update.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function first checks if the module exists in the graph and logs a message if it does not.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content=\"It then checks if the module's file exists or if the force removal flag is set, and if so, deletes the module and returns a normal update.\", metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content=\"If the module's file exists and the force removal flag is not set, the function gets the module's sources, restores the old module's state, and loads the new module's graph.\", metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content=\"It then checks for any blocking errors and restores the old module's state if necessary.\", metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='If no blocking errors are found, the function processes the changed file, merges the old and new ASTs, and performs type checking.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='It also logs messages and adds statistics to the build manager.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `find_relative_leaf_module` is used to find a module that directly imports no other modules in the list.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `delete_module` is used to delete a module from the build process.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `dedupe_modules` is used to remove duplicate modules from a list.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `get_module_to_path_map` is used to get a map of modules to their paths.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `get_sources` is used to get the sources of a module.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'}),\n",
       " Document(page_content='The function `calculate_active_triggers` is used to determine activated triggers by comparing old and new symbol tables.', metadata={'file_name': 'update.py', 'file_type': '.py', 'text': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}', 'code': 'class BlockedUpdate(NamedTuple):\\n    module: str\\n    path: str\\n    remaining: list[tuple[str, str]]\\n    messages: list[str]\\n\\n\\nUpdateResult: _TypeAlias = Union[NormalUpdate, BlockedUpdate]\\n\\n\\ndef update_module_isolated(\\n    module: str,\\n    path: str,\\n    manager: BuildManager,\\n    previous_modules: dict[str, str],\\n    graph: Graph,\\n    force_removed: bool,\\n    followed: bool,\\n) -> UpdateResult:\\n    \"\"\"Build a new version of one changed module only.\\n\\n    Don\\'t propagate changes to elsewhere in the program. Raise CompileError on\\n    encountering a blocking error.\\n\\n    Args:\\n        module: Changed module (modified, created or deleted)\\n        path: Path of the changed module\\n        manager: Build manager\\n        graph: Build graph\\n        force_removed: If True, consider the module removed from the build even it the\\n            file exists\\n\\n    Returns a named tuple describing the result (see above for details).\\n    \"\"\"\\n    if module not in graph:\\n        manager.log_fine_grained(f\"new module {module!r}\")\\n\\n    if not manager.fscache.isfile(path) or force_removed:\\n        delete_module(module, path, graph, manager)\\n        return NormalUpdate(module, path, [], None)\\n\\n    sources = get_sources(manager.fscache, previous_modules, [(module, path)], followed)\\n\\n    if module in manager.missing_modules:\\n        manager.missing_modules.remove(module)\\n\\n    orig_module = module\\n    orig_state = graph.get(module)\\n    orig_tree = manager.modules.get(module)\\n\\n    def restore(ids: list[str]) -> None:\\n        # For each of the modules in ids, restore that id\\'s old\\n        # manager.modules and graphs entries. (Except for the original\\n        # module, this means deleting them.)\\n        for id in ids:\\n            if id == orig_module and orig_tree:\\n                manager.modules[id] = orig_tree\\n            elif id in manager.modules:\\n                del manager.modules[id]\\n            if id == orig_module and orig_state:\\n                graph[id] = orig_state\\n            elif id in graph:\\n                del graph[id]\\n\\n    new_modules: list[State] = []\\n    try:\\n        if module in graph:\\n            del graph[module]\\n        load_graph(sources, manager, graph, new_modules)\\n    except CompileError as err:\\n        # Parse error somewhere in the program -- a blocker\\n        assert err.module_with_blocker\\n        restore([module] + [st.id for st in new_modules])\\n        return BlockedUpdate(err.module_with_blocker, path, [], err.messages)\\n\\n    # Reparsing the file may have brought in dependencies that we\\n    # didn\\'t have before. Make sure that they are loaded to restore\\n    # the invariant that a module having a loaded tree implies that\\n    # its dependencies do as well.\\n    ensure_trees_loaded(manager, graph, graph[module].dependencies)\\n\\n    # Find any other modules brought in by imports.\\n    changed_modules = [(st.id, st.xpath) for st in new_modules]\\n\\n    # If there are multiple modules to process, only process one of them and return\\n    # the remaining ones to the caller.\\n    if len(changed_modules) > 1:\\n        # As an optimization, look for a module that imports no other changed modules.\\n        module, path = find_relative_leaf_module(changed_modules, graph)\\n        changed_modules.remove((module, path))\\n        remaining_modules = changed_modules\\n        # The remaining modules haven\\'t been processed yet so drop them.\\n        restore([id for id, _ in remaining_modules])\\n        manager.log_fine_grained(f\"--> {module!r} (newly imported)\")\\n    else:\\n        remaining_modules = []\\n\\n    state = graph[module]\\n\\n    # Process the changed file.\\n    state.parse_file()\\n    assert state.tree is not None, \"file must be at least parsed\"\\n    t0 = time.time()\\n    try:\\n        semantic_analysis_for_scc(graph, [state.id], manager.errors)\\n    except CompileError as err:\\n        # There was a blocking error, so module AST is incomplete. Restore old modules.\\n        restore([module])\\n        return BlockedUpdate(module, path, remaining_modules, err.messages)\\n\\n    # Merge old and new ASTs.\\n    new_modules_dict: dict[str, MypyFile | None] = {module: state.tree}\\n    replace_modules_with_new_variants(manager, graph, {orig_module: orig_tree}, new_modules_dict)\\n\\n    t1 = time.time()\\n    # Perform type checking.\\n    state.type_checker().reset()\\n    state.type_check_first_pass()\\n    state.type_check_second_pass()\\n    state.detect_possibly_undefined_vars()\\n    t2 = time.time()\\n    state.finish_passes()\\n    t3 = time.time()\\n    manager.add_stats(semanal_time=t1 - t0, typecheck_time=t2 - t1, finish_passes_time=t3 - t2)\\n\\n    graph[module] = state\\n\\n    return NormalUpdate(module, path, remaining_modules, state.tree)\\n\\n\\ndef find_relative_leaf_module(modules: list[tuple[str, str]], graph: Graph) -> tuple[str, str]:\\n    \"\"\"Find a module in a list that directly imports no other module in the list.\\n\\n    If no such module exists, return the lexicographically first module from the list.\\n    Always return one of the items in the modules list.\\n\\n    NOTE: If both \\'abc\\' and \\'typing\\' have changed, an effect of the above rule is that\\n        we prefer \\'abc\\', even if both are in the same SCC. This works around a false\\n        positive in \\'typing\\', at least in tests.\\n\\n    Args:\\n        modules: List of (module, path) tuples (non-empty)\\n        graph: Program import graph that contains all modules in the module list\\n    \"\"\"\\n    assert modules\\n    # Sort for repeatable results.\\n    modules = sorted(modules)\\n    module_set = {module for module, _ in modules}\\n    for module, path in modules:\\n        state = graph[module]\\n        if len(set(state.dependencies) & module_set) == 0:\\n            # Found it!\\n            return module, path\\n    # Could not find any. Just return the first module (by lexicographic order).\\n    return modules[0]\\n\\n\\ndef delete_module(module_id: str, path: str, graph: Graph, manager: BuildManager) -> None:\\n    manager.log_fine_grained(f\"delete module {module_id!r}\")\\n    # TODO: Remove deps for the module (this only affects memory use, not correctness)\\n    if module_id in graph:\\n        del graph[module_id]\\n    if module_id in manager.modules:\\n        del manager.modules[module_id]\\n    components = module_id.split(\".\")\\n    if len(components) > 1:\\n        # Delete reference to module in parent module.\\n        parent_id = \".\".join(components[:-1])\\n        # If parent module is ignored, it won\\'t be included in the modules dictionary.\\n        if parent_id in manager.modules:\\n            parent = manager.modules[parent_id]\\n            if components[-1] in parent.names:\\n                del parent.names[components[-1]]\\n    # If the module is removed from the build but still exists, then\\n    # we mark it as missing so that it will get picked up by import from still.\\n    if manager.fscache.isfile(path):\\n        manager.missing_modules.add(module_id)\\n\\n\\ndef dedupe_modules(modules: list[tuple[str, str]]) -> list[tuple[str, str]]:\\n    seen: set[str] = set()\\n    result = []\\n    for id, path in modules:\\n        if id not in seen:\\n            seen.add(id)\\n            result.append((id, path))\\n    return result\\n\\n\\ndef get_module_to_path_map(graph: Graph) -> dict[str, str]:\\n    return {module: node.xpath for module, node in graph.items()}\\n\\n\\ndef get_sources(\\n    fscache: FileSystemCache,\\n    modules: dict[str, str],\\n    changed_modules: list[tuple[str, str]],\\n    followed: bool,\\n) -> list[BuildSource]:\\n    sources = []\\n    for id, path in changed_modules:\\n        if fscache.isfile(path):\\n            sources.append(BuildSource(path, id, None, followed=followed))\\n    return sources\\n\\n\\ndef calculate_active_triggers(\\n    manager: BuildManager,\\n    old_snapshots: dict[str, dict[str, SymbolSnapshot]],\\n    new_modules: dict[str, MypyFile | None],\\n) -> set[str]:\\n    \"\"\"Determine activated triggers by comparing old and new symbol tables.\\n\\n    For example, if only the signature of function m.f is different in the new\\n    symbol table, return {\\'<m.f>\\'}.\\n    \"\"\"\\n    names: set[str] = set()\\n    for id in new_modules:\\n        snapshot1 = old_snapshots.get(id)\\n        if snapshot1 is None:\\n            names.add(id)\\n            snapshot1 = {}\\n        new = new_modules[id]\\n        if new is None:\\n            snapshot2 = snapshot_symbol_table(id, SymbolTable())\\n            names.add(id)\\n        else:\\n            snapshot2 = snapshot_symbol_table(id, new.names)\\n        diff = compare_symbol_table_snapshots(id, snapshot1, snapshot2)\\n        package_nesting_level = id.count(\".\")\\n        for item in diff.copy():\\n            if item.count(\".\") <= package_nesting_level + 1 and item.split(\".\")[-1] not in (\\n                \"__builtins__\",\\n                \"__file__\",\\n                \"__name__\",\\n                \"__package__\",\\n                \"__doc__\",\\n            ):\\n                # Activate catch-all wildcard trigger for top-level module changes (used for\\n                # \"from m import *\"). This also gets triggered by changes to module-private\\n                # entries, but as these unneeded dependencies only result in extra processing,\\n                # it\\'s a minor problem.\\n                #\\n                # TODO: Some __* names cause mistriggers. Fix the underlying issue instead of\\n                #     special casing them here.\\n                diff.add(id + WILDCARD_TAG)\\n            if item.count(\".\") > package_nesting_level + 1:\\n                # These are for changes within classes, used by protocols.\\n                diff.add(item.rsplit(\".\", 1)[0] + WILDCARD_TAG)\\n\\n        names |= diff\\n    return {make_trigger(name) for name in names}'})]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'contextvars.pyi',\n",
       " 'file_type': '.pyi',\n",
       " 'text': 'import sys\\nfrom collections.abc import Callable, Iterator, Mapping\\nfrom typing import Any, ClassVar, Generic, TypeVar, final, overload\\nfrom typing_extensions import ParamSpec\\n\\nif sys.version_info >= (3, 9):\\n    from types import GenericAlias\\n\\n__all__ = (\"Context\", \"ContextVar\", \"Token\", \"copy_context\")\\n\\n_T = TypeVar(\"_T\")\\n_D = TypeVar(\"_D\")\\n_P = ParamSpec(\"_P\")\\n\\n@final\\nclass ContextVar(Generic[_T]):\\n    @overload\\n    def __init__(self, name: str) -> None: ...\\n    @overload\\n    def __init__(self, name: str, *, default: _T) -> None: ...\\n    def __hash__(self) -> int: ...\\n    @property\\n    def name(self) -> str: ...\\n    @overload\\n    def get(self) -> _T: ...\\n    @overload\\n    def get(self, default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, default: _D, /) -> _D | _T: ...\\n    def set(self, value: _T, /) -> Token[_T]: ...\\n    def reset(self, token: Token[_T], /) -> None: ...\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\n@final\\nclass Token(Generic[_T]):\\n    @property\\n    def var(self) -> ContextVar[_T]: ...\\n    @property\\n    def old_value(self) -> Any: ...  # returns either _T or MISSING, but that\\'s hard to express\\n    MISSING: ClassVar[object]\\n    if sys.version_info >= (3, 9):\\n        def __class_getitem__(cls, item: Any, /) -> GenericAlias: ...\\n\\ndef copy_context() -> Context: ...\\n\\n# It doesn\\'t make sense to make this generic, because for most Contexts each ContextVar will have\\n# a different value.\\n@final\\nclass Context(Mapping[ContextVar[Any], Any]):\\n    def __init__(self) -> None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: None = None, /) -> _T | None: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _T, /) -> _T: ...\\n    @overload\\n    def get(self, key: ContextVar[_T], default: _D, /) -> _T | _D: ...\\n    def run(self, callable: Callable[_P, _T], *args: _P.args, **kwargs: _P.kwargs) -> _T: ...\\n    def copy(self) -> Context: ...\\n    def __getitem__(self, key: ContextVar[_T], /) -> _T: ...\\n    def __iter__(self) -> Iterator[ContextVar[Any]]: ...\\n    def __len__(self) -> int: ...\\n    def __eq__(self, value: object, /) -> bool: ...'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, out in enumerate(pythonOutput[:10]):\n",
    "    out.metadata['code'] = out.page_content\n",
    "    out.metadata['summaries']\n",
    "    out.page_content(all_summaries[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pythonOutput[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anandtyagi/.pyenv/versions/3.11.8/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'get_doc_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m PineconeVectorStore(pinecone_index\u001b[38;5;241m=\u001b[39mpci)\n\u001b[1;32m      7\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m----> 8\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpythonOutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/llama_index/core/indices/base.py:136\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex_construction\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m--> 136\u001b[0m         docstore\u001b[38;5;241m.\u001b[39mset_document_hash(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_doc_id\u001b[49m(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    138\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    139\u001b[0m         documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         transformations,\n\u001b[1;32m    141\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    146\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    147\u001b[0m         storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    153\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'get_doc_id'"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=pci)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    pythonOutput, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2622 [00:18<6:48:23,  9.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m all_summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(pythonOutput):\n\u001b[0;32m----> 7\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summaries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     all_summaries\u001b[38;5;241m.\u001b[39mappend(resp)\n",
      "Cell \u001b[0;32mIn[72], line 16\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_summaries\u001b[39m(document):\n\u001b[0;32m---> 16\u001b[0m \tresponse \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are an expert coder that creates sumamries of code files.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43mChatMessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTake this code file and create a list of summaries for each class and function in the block of code. Here is the code: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpythonOutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\t\t\t\t\u001b[49m\u001b[43mrole\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\t\t\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama-3-70b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletionResponseFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson_object\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/octoai/text_gen/client.py:371\u001b[0m, in \u001b[0;36mTextGenClient.create_chat_completion\u001b[0;34m(self, messages, model, frequency_penalty, ignore_eos, logit_bias, loglikelihood, logprobs, max_tokens, n, peft, presence_penalty, repetition_penalty, response_format, stop, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, request_options)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_chat_completion\u001b[39m(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     request_options: typing\u001b[38;5;241m.\u001b[39mOptional[RequestOptions] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletionResponse:\n\u001b[1;32m    278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    Create a Chat Completion.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    )\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m     _response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore_eos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_eos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloglikelihood\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloglikelihood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepetition_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43momit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOMIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m _response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/octoai/core/http_client.py:171\u001b[0m, in \u001b[0;36mHttpClient.request\u001b[0;34m(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\u001b[0m\n\u001b[1;32m    164\u001b[0m base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_base_url(base_url)\n\u001b[1;32m    165\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    166\u001b[0m     request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout_in_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_timeout\n\u001b[1;32m    169\u001b[0m )\n\u001b[0;32m--> 171\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttpx_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_headers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                \u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madditional_query_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    192\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                    \u001b[49m\u001b[43momit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_filter_request_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_filter_request_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_file_dict_to_httpx_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m max_retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m request_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_retry(response\u001b[38;5;241m=\u001b[39mresponse):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "all_summaries = []\n",
    "\n",
    "\n",
    "for doc in tqdm.tqdm(pythonOutput):\n",
    "    resp = generate_summaries(doc)\n",
    "    all_summaries.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
